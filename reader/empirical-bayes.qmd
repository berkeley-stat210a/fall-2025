---
title: "The James-Stein Estimator"
format:
  html:
    toc: true
    number-sections: true
editor: 
  markdown: 
    wrap: 72
---

{{< include latex-macros.tex >}}

## Gaussian sequence model

Recall that we have discussed a variety of estimators for
$\theta \in \RR^d$ in the *Gaussian sequence model*

$$X \sim N_d(\theta, I_d)$$

Note that this model is somewhat more general than it appears. If
$X_1,\ldots,X_n \simiid N_d(\theta, \sigma^2 I_d)$ for known
$\sigma^2> 0$, we could make a sufficiency reduction to obtain

$$Z = \frac{1}{\sigma\sqrt{n}} \sum\_i X_i \sim N_d(\theta, I_d).$$ For
simplicity we will discuss the \`\`vanilla'' version here, but we can
always translate our results to the more general setting via this
transformation.

We'll generally assume in what follows that the loss we care about is
the squared error loss, summed over the coordinates:\
$$L(\theta, d) =
\|\delta(X) - \theta\|\^2 = \sum\_j (\delta\_j(X) - \theta\_j)\^2$$

The most obvious estimator is $\delta_0(X) = X$ itself, which we could justify in a variety of ways: we've shown that it is the UMVU estimator for $\theta$ and also the objective Bayes estimator, since the flat prior on $\theta$ coincides with the Jeffreys prior (as it does for any location model). It also happens to be the maximum likelihood estimator (MLE), which we'll discuss later in the course.

### Bayes estimators 

If we introduce the Bayesian prior $\theta_i \simiid N(0,\tau^2)$ then we have seen that we arrive at the Bayes estimator $\frac{\tau^2}{1+\tau^2}X$.

We can think of this as a tuning parameter for a generic *linear shrinkage estimator*
$$\delta_\zeta(X) = (1-\zeta)X,$$
where $\zeta \in [0,1]$ is in effect a tuning parameter we will call the *shrinkage parameter*. Taking $\zeta = 0$ corresponds to using $X$ as our estimator for $\theta$, and taking $\zeta = 1/(1+\tau^2)$ corresponds to the Bayes estimator where $\tau^2$ is known.

If we aren't sure which $\zeta$ to use, for example because we have some *a priori* uncertainty about $\tau^2$, we can try to estimate it from the data using hierarchical Bayes, which we've seen would give the final estimator
$$\delta(X) = (1 - \EE[\zeta \mid X]) X = \delta_{\hat\zeta_{\text{Bayes}}(X)}(X),$$
so we are in effect estimating $\zeta$ from the whole data set and then plugging it in as a data-adaptive tuning parameter.

The hierarchical Bayes estimator uses a Bayes estimator for $\zeta$, but if we take an empirical Bayes approach we could try other estimators, such as the MLE or UMVU. If $d \geq 3$ then the UMVU estimator for $\zeta$ is
$$\hat{\zeta}_{\text{UMVU}}(X) = \frac{d-2}{\|X\|^2},$$
which we can verify using the identity 
$$\EE[1/Y] = \frac{1}{d-2}, \quad \text{ if } Y \sim \chi_d^2 = \text{Gamma}(d/2, 2), \text{ for } d > 2,$$
which is proved in the handwritten notes. Plugging in $\hat\zeta_\text{UMVU}$ results in an estimator called the *James-Stein* estimator, 
$$ \delta_{\text{JS}}(X)= \left(1 - \frac{d-2}{\|X\|^2}\right)X = \delta_{\hat\zeta_{\text{UMVU}}}(X) $$

### James-Stein Paradox

While the James-Stein estimator can be motivated as an empirical Bayes estimator, it is surprisingly good even without making any Bayesian assumptions at all.

For $d \geq 3$, the estimator $X$ is actually *inadmissible* as an estimator of $\theta$ under squared error loss:

$$\text{MSE}(\theta, \delta_{JS}) < \text{MSE}(\theta, X) \quad \text{ for all } \theta \in \mathbb{R}^d.$$

It is not surprising for a Bayes estimator to beat the UMVU estimator *an average* with respect to some prior, but this result holds for *every fixed value* of the parameter $\theta$.

In fact, since there is nothing special about shrinking towards $0$. We could use a version of the estimator that shrinks toward any other $\theta_0 \in \RR^d$, i.e.
$$ \tilde delta(X) = \theta_0 + \left(1 - \frac{d-2}{\|X - \theta_0\|^2}\right) (X - \theta_0)$$.
This also dominates $\delta_0$ because it is just the James-Stein estimator we'd get if we made the substitution 
$$Y = X - \theta_0 \sim N_d(\mu, I_d), \quad \text{ for } \mu = \theta - \theta_0.$$
The translation-invariance of the Gaussian location model means that the James-Stein estimator for $\mu$ using $Y$ also dominates the estimator $\hat\mu_0(Y) = Y$, which corresponds to the estimator $\delta_0(X) = \hat\mu_0 + \theta_0 = X$ for $\theta$.

This result was received as a shock in the 1950s when it first came out. It was regarded for a long time as a curiosity, but it was eventually understood to carry the deep implication that shrinkage makes sense, especially in higher-dimensional problems, even when we don't have a Bayes justification for it.


### Linear shrinkage estimators

Even without introducing a Bayesian prior for $\theta$, we can motivate our linear shrinkage estimator purely from the perspective of trading a bit of bias for a reduction in variance.

We can start by calculating the MSE (considered as a purely frequentist risk function) for a single coordinate, using the bias-variance tradeoff:
$$
\begin{aligned}
\EE_\theta[(\theta - \delta_i(X))^2]
&= (\theta_i - \EE_\theta (1-\zeta)X_i)^2 + \text{Var}_\theta (1-\zeta)X_i\\
&= (\zeta\theta_i)^2 + (1-\zeta)^2
\end{aligned}
$$
Summing over the $d$ coordinates gives
$$\text{MSE}(\theta; \delta) = \zeta^2\|\theta\|^2 + d(1-\zeta)^2,$$
where the first term represents the squared bias and the second is the variance.

Note that the risk is a quadratic in $\zeta$ with positive second derivative, so we can minimize it by setting
$$0 = \frac{d}{d\zeta}\text{MSE}(\theta) = 2\zeta\|\theta\|^2 - 2(1-\zeta)d,$$
leading to 
$$\zeta^*(\theta) = \frac{d}{d+\|\theta\|^2} = \frac{1}{1+\|\theta\|^2/d},$$
which looks remarkably similar to $\frac{1}{1+\tau^2}$, which is the Bayes-optimal $\zeta$ under the Gaussian prior from the last section.

One thing to notice is that $\zeta^*(\theta) > 0$, so a small amount of shrinkage helps. But the correct amount of shrinkage depends on $\|\theta\|^2$: if $\|\theta\|^2 \to \infty$, the correct amount of shrinkage goes to $0$, so any fixed $\zeta$ would overshoot for some $\theta$ parameters.

It turns out the James-Stein estimator manages to estimate the correct amount of shrinkage from the data, in such a way that we avoid overshooting most of the time, and thereby improve on the MSE for *any* $\theta$.

To understand why, we need a general way to calculate the MSE for an estimator with an adaptive $\hat\zeta(X)$. Stein's unbiased risk estimator will give us that.

## Stein's Unbiased Risk Estimator

### Stein's Lemma

The first ingredient in finding the MSE of $\delta_\text{JS}$ is a lemma called *Stein's Lemma*:

**Theorem (Stein's lemma, univariate):** Suppose $X \sim N(\theta, \sigma^2)$, and that $h:\; \RR \to \RR$ is differentiable, with $\EE|\dot{h}(X)| < \infty$. Then we have
$$\Cov(X, h(X)) = \EE[(X-\theta)h(X)] = \sigma^2\EE[\dot{h}(X)].$$

*Proof*: 


Next, we will do the calculation for $\theta = 0$ and $\sigma^2 = 1$. Then
$$\EE[Xh(X)] = \int_{-\infty}^\infty xh(x)\phi(x)\,dx = \int_{-\infty}^\infty \dot{h}(x)\phi(x)\,dx,$$
where we've used integration by parts:
$$\dot{\phi}(x) = \frac{d}{dx} \frac{1}{\sqrt{2\pi}}e^{-x^2/2} = -x \frac{1}{\sqrt{2\pi}}e^{-x^2/2} = -x\phi(x).$$
(A slightly more rigorous version is in the handwritten notes: we can assume wlog $h(0) = 0$ because otherwise we could center it by using $k(X) = h(X) - h(0)$, which has the same covariance with $X$. Then we can break up the integral into an integral from $0$ to $\infty$ and another from $-\infty$ to $0$, and do integration by parts a bit more carefully for each.)

For more general $\theta$, we can write $X = \theta + \sigma Z$, where $Z \sim N(0,1)$. Then applying the result for $k(z) = h(\theta + \sigma z)$, we have

$$\EE[(X-\theta) h(X)] = \sigma\EE[Zh(\theta + \sigma Z)] = \sigma^2\EE[\dot{h}(\theta + \sigma Z)] = \sigma^2\EE[\dot{h}(X)],$$
giving the general result.

We will need the multivariate version of Stein's lemma. For a function $h:\; \RR^d \to \RR^d$, define the Jacobian matrix $Dh \in \RR^{d\times d}$ by
$$(Dh(x))_{ij} = \frac{\partial h_i}{\partial x_j}(x).$$

Define the **Frobenius norm** $A \in \RR^{d\times d}$ as 
$$\|A\|_F = (\sum_{ij} A_{ij}^2)^{1/2}.$$

Now we can state our theorem:

**Theorem (Stein's lemma, multivariate):** Assume $X \sim N_d(\theta; \sigma^2 I_d)$, and $h:\;\RR^d \to \RR^d$ is differentiable with $\EE\|Dh(X)\|_F < \infty$. Then 
$$ \EE[(X-\theta)'h(X)] = \sigma^2 \EE \text{tr}(Dh(X)) = \sigma^2 \sum_i \EE \frac{\partial h_i}{\partial x_i} (X).$$ 

The proof follows easily from the proof of the univariate version, and appears in the handwritten notes.

### Stein's unbaised risk estimator (SURE)

### Risk of James-Stein

## Empirical Bayes

### 

### Common Situation in Hierarchical Bayes Models

1.  $\theta \sim G$, one draw: hard to justify prior
2.  Lots of info: prior doesn't matter
3.  $\theta_i \sim G$, only $X_i$ informative: prior helps
4.  Many draws: can check fit

$$X_i \sim p_{\theta_i}(x), \quad i = 1,\ldots,d$$

### Hybrid Approach

Treat $G$ as fixed:

1.  Estimate $G$ based on observed data
2.  Plug in $\hat{G}$ as though known

### Example

$\theta_i \sim N(0, \tau^2)$, $\tau^2$ fixed unknown
$X_i|\theta_i \sim N(\theta_i, 1)$, $i = 1,\ldots,d$

Bayes estimator if we knew $\tau^2$ is:

$$\delta(X) = \frac{\tau^2}{\tau^2 + 1}X_i$$

$\tau^2$ is sufficient.

To estimate $\tau^2$, use $X \sim N(0, \tau^2 I_d + I_d)$:

$$\mathbb{E}\|X\|^2 = d(\tau^2 + 1)$$

$$\hat{\tau}^2 = \max\{\frac{1}{d}\|X\|^2 - 1, 0\}$$

Plug in: $\hat{\delta}(X) = (1 - \frac{d}{\|X\|^2})_+ X_i$

If $d$ large, should be near optimal.

## James-Stein Estimator

$$\delta_{JS}(X) = (1 - \frac{d-2}{\|X\|^2})X$$

Proof: If $Y \sim \text{Gamma}(\frac{d}{2}, \frac{1}{2})$, then:

$$\mathbb{P}(Y > \frac{d-2}{2}) = 1$$

Now use $f(x) = x - \frac{d-2}{x}$,
$\frac{1}{2}\|X\|^2 \sim \text{Gamma}(\frac{d}{2}, \frac{1}{2})$

$$\mathbb{E}[f(\frac{1}{2}\|X\|^2)] > f(\mathbb{E}[\frac{1}{2}\|X\|^2]) = \frac{d}{2} - \frac{d-2}{\frac{d}{2}} = 1$$

### James-Stein Paradox

For $d \geq 3$, the sample mean $\bar{X} = \frac{1}{n}\sum X_i$ is
inadmissible as an estimator of $\theta$ under squared error loss.

For $\delta_{JS}(X) = (1 - \frac{d-2}{\|X\|^2})X$:

$$\text{MSE}(\theta, \delta_{JS}) < \text{MSE}(\theta, \bar{X}) \quad \forall \theta \in \mathbb{R}^d$$

Notes: - $\bar{X}$ is UMVU, Minimax, objective Bayes - Might as well
take $n=1$ (Sufficiency reduction) - This result holds without
assumption of Bayes model on $\theta$: true for
$\theta = (50, 10, 94, \ldots)$ - Nothing special about 0: for any
$\theta_0 \in \mathbb{R}^d$,
$\delta(X) = \theta_0 + (1 - \frac{d-2}{\|X-\theta_0\|^2})(X-\theta_0)$
also dominates $X$

Deep implication: shrinkage makes sense even without Bayes
justification.

### General Form

Let $\delta(X) = (1 - \frac{c}{\|X\|^2})X$, $c$ is tuning parameter

$$R(\theta, \delta) = \mathbb{E}_\theta\|\delta(X) - \theta\|^2 = \mathbb{E}_\theta\|X - \theta\|^2 + \mathbb{E}_\theta[\frac{c^2}{\|X\|^2} - 2c]$$

What is optimal $c$?

$$R(\theta, \delta) = d + \mathbb{E}_\theta[\frac{c^2}{\|X\|^2}] - 2c$$

$$\frac{\partial R}{\partial c} = 2\mathbb{E}_\theta[\frac{c}{\|X\|^2}] - 2 = 0$$

$$c = \frac{\mathbb{E}_\theta[\|X\|^2]}{\mathbb{E}_\theta[\frac{1}{\|X\|^2}]}$$

$c$ always \> 0, but → 0 as $\|\theta\| → \infty$

What if we estimate $c$? How does adaptivity of $c(X)$ affect MSE?

## Stein's Lemma

Useful tool for computing/estimating risk in Gaussian estimation
problems.

### Theorem (Stein's Lemma - Univariate)

Suppose $X \sim N(\theta, \sigma^2)$ $h: \mathbb{R} → \mathbb{R}$
differentiable, $\mathbb{E}|h'(X)| < \infty$

Then $\mathbb{E}[(X-\theta)h(X)] = \sigma^2\mathbb{E}[h'(X)]$

$\text{Cov}(X, h(X)) = \sigma^2\mathbb{E}[h'(X)]$

Proof: Note we can assume w.l.o.g. $h(0) = 0$ (why?) First assume
$\theta = 0$, $\sigma^2 = 1$

$$\mathbb{E}[Xh(X)] = \int xh(x)\phi(x)dx = \int xh(x)\phi(x)dx - \int h(x)\phi'(x)dx = \int h'(x)\phi(x)dx$$

In the last step we have used $\phi'(x) = -x\phi(x)$

Similar argument shows $\int h(x)\phi(x)dx = \int h'(x)\phi(x)dx$

Result holds for $\theta = 0$, $\sigma^2 = 1$

General $\theta$, $\sigma^2 \neq 1$: write $X = \theta + \sigma Z$,
$Z \sim N(0,1)$

$$\mathbb{E}[(X-\theta)h(X)] = \sigma\mathbb{E}[Zh(\theta+\sigma Z)] = \sigma^2\mathbb{E}[h'(\theta+\sigma Z)] = \sigma^2\mathbb{E}[h'(X)]$$

### Multivariate Version

Define Frobenius norm:
$\|A\|_F^2 = \sum_{i,j} A_{ij}^2 = \text{tr}(A^TA)$

### Theorem (Stein's Lemma - Multivariate)

$X \sim N_d(\theta, \sigma^2 I_d)$, $\theta \in \mathbb{R}^d$
$h: \mathbb{R}^d → \mathbb{R}^d$ differentiable,
$\mathbb{E}\|Dh(X)\|_F < \infty$

Then
$\mathbb{E}[(X-\theta)^Th(X)] = \sigma^2\mathbb{E}[\text{tr}(Dh(X))]$

$\mathbb{E}[(X-\theta)h(X)^T] = \sigma^2\mathbb{E}[Dh(X)]$

Proof:

$$\mathbb{E}[(X_i-\theta_i)h_i(X)] = \mathbb{E}[\mathbb{E}[(X_i-\theta_i)h_i(X)|X_{-i}]]$$
$$= \sigma^2\mathbb{E}[\frac{\partial h_i}{\partial x_i}(X)]$$

## Stein's Unbiased Risk Estimator (SURE)

Unbiased estimator of the MSE of any $\delta(X)$

Apply Stein's Lemma with $h(x) = X - \delta(x)$

Assume $\sigma^2 = 1$:

$$R(\theta, \delta) = \mathbb{E}_\theta\|\delta(X) - \theta\|^2 = \mathbb{E}_\theta\|X - \theta\|^2 + \mathbb{E}_\theta\|\delta(X) - X\|^2 - 2\mathbb{E}_\theta[(X-\theta)^T(X-\delta(X))]$$
$$= d + \mathbb{E}_\theta\|\delta(X) - X\|^2 - 2\mathbb{E}_\theta[\text{tr}(D(X-\delta(X)))]$$

$$\hat{R}(X) = d + \|\delta(X) - X\|^2 - 2\text{tr}(D\delta(X))$$

is unbiased for the MSE estimator $\delta(X)$

Only depends on X

Can also compute MSE via $R = \mathbb{E}_\theta[\hat{R}]$

$$\mathbb{E}_\theta[\|\delta(X) - h(X)\|^2] = \mathbb{E}_\theta[\|\delta(X) - \theta\|^2] + \mathbb{E}_\theta[\|h(X) - \theta\|^2] - 2\mathbb{E}_\theta[(\delta(X) - \theta)^T(h(X) - \theta)]$$

$$R = d + R(\theta, \delta) - 2\mathbb{E}_\theta[\text{tr}(D\delta(X))]$$

Example: $\delta(X) = (1-c)X$ for fixed $c$

$h(x) = cX$, $Dh = cI_d$

$\hat{R} = d + c^2\|X\|^2 - 2cd$

### Risk of James-Stein

$\delta_{JS}(X) = (1 - \frac{d-2}{\|X\|^2})X$

$h(X) = \frac{d-2}{\|X\|^2}X$,
$Dh(X) = \frac{d-2}{\|X\|^2}I_d - 2(d-2)\frac{XX^T}{\|X\|^4}$

$\|h(X)\|^2 = \frac{(d-2)^2}{\|X\|^2}$

$$\text{tr}(Dh(X)) = \frac{d(d-2)}{\|X\|^2} - \frac{2(d-2)}{\|X\|^2} = \frac{(d-2)^2}{\|X\|^2}$$

$$\hat{R} = d + \frac{(d-2)^2}{\|X\|^2} - 2\frac{(d-2)^2}{\|X\|^2} = d - \frac{(d-2)^2}{\|X\|^2}$$

$$R(\theta) = \mathbb{E}_\theta[\hat{R}] = d - (d-2)^2\mathbb{E}_\theta[\frac{1}{\|X\|^2}]$$

If $\theta = 0$ then $\mathbb{E}_0[\frac{1}{\|X\|^2}] = \frac{1}{d-2}$

$$R(0) = d - (d-2) = 2$$

Possibly surprising

If $\theta \neq 0$ then
$\mathbb{E}_\theta[\frac{1}{\|X\|^2}] < \frac{1}{\|\theta\|^2}$

$$R(\theta) < d - \frac{(d-2)^2}{\|\theta\|^2 + d}$$

Smaller and smaller advantage, but always better

Note: $\delta_{JS}(X)$ also inadmissible

$\delta_{+}(X) = (1 - \frac{d-2}{\|X\|^2})_+ X$ is strictly better

Practically more useful version:

$$\delta_{JS+}(X) = (1 - \frac{d-3}{\|X\|^2})_+ X$$

Dominates $\delta(X) = X$ for $d \geq 4$

Taken to logical extreme, suggestion seems dumb: should everyone at
Berkeley pool their estimates?

Note: $\mathbb{E}\|\hat{\theta}\|^2$ is improved, but
$\mathbb{E}[(\hat{\theta}_i - \theta_i)^2]$ may get worse for individual
coordinates.
