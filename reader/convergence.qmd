---
title: "Asymptotic Theory: Convergence, Continuous Mapping, and Delta Method"
date: "2023-11-02"
format:
  html:
    toc: true
    number-sections: true
---

{{< include latex-macros.tex >}}

## Introduction to Asymptotic Theory

So far, everything has been finite sample, often using special properties of model $\cP$ (e.g., exponential family) to do exact calculations. For generic models, exact calculations may be intractable or impossible. However, we may be able to approximate our problem with a simpler problem in which calculations are easy.

Typically, we approximate by Gaussian by taking the limit as observations $n \to \infty$. But this is only interesting if the approximation is good for reasonable sample sizes.

## Convergence

Let $X_1, X_2, \ldots \in \mathbb{R}^d$ be a sequence of random vectors. We care about two kinds of convergence:

1. Convergence in probability: $X_n \to$ a constant
2. Convergence in distribution: $X_n \to N(0, I_d)$ (usually)

### Convergence in Probability

We say the sequence converges in probability to $c \in \mathbb{R}^d$ ($X_n \xrightarrow{p} c$) if:

$$\mathbb{P}(\|X_n - c\| > \epsilon) \to 0 \quad \forall \epsilon > 0$$

(Could really be any distance on any $\cX$)

Can converge to a r.v. $X$ too, but we don't need this.

### Convergence in Distribution

We say the sequence converges in distribution to random variable $X$ ($X_n \xrightarrow{d} X$) if:

$$\mathbb{E}[f(X_n)] \to \mathbb{E}[f(X)] \text{ for all bounded continuous } f: \cX \to \mathbb{R}$$

Theorem: $X_n, X \in \mathbb{R}$. Fix $\mathbb{P}(X = x) = 0$. Let $F_n(x) = \mathbb{P}(X_n \leq x)$, $F(x) = \mathbb{P}(X \leq x)$.
Then $X_n \xrightarrow{d} X$ iff $F_n(x) \to F(x)$ $\forall x: F$ is continuous at $x$.

Also known as weak convergence.

### Example

If $X_n \xrightarrow{d} X \sim g$, then $X_n \xrightarrow{d} X$:

$$F_n(x) = \begin{cases}
1 & \text{if } x > 0 \\
1 - \frac{1}{n} & \text{if } x = 0 \\
0 & \text{if } x < 0
\end{cases}$$

$$F(x) = \begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}$$

### Proof: $X_n \xrightarrow{p} c \implies X_n \xrightarrow{d} c$

Let $f_\epsilon(x) = \max\{1 - \frac{\|x-c\|}{\epsilon}, 0\}$. Then $\forall \epsilon > 0$:

$$\mathbb{P}(\|X_n - c\| > \epsilon) \leq \mathbb{E}[1 - f_\epsilon(X_n)] \to 0$$

$f$ bounded continuous. Note $\mathbb{E}[f(c)] = f(c)$.

$\forall \epsilon > 0$, $\exists \delta > 0$ s.t. $\|x - c\| < \delta \implies |f(x) - f(c)| < \epsilon$

$$|\mathbb{E}[f(X_n)] - f(c)| \leq |\mathbb{E}[f(X_n) - f(c)]1_{\|X_n - c\| < \delta}| + |\mathbb{E}[(f(X_n) - f(c))1_{\|X_n - c\| \geq \delta}]|$$
$$\leq \epsilon + 2\sup |f| \cdot \mathbb{P}(\|X_n - c\| \geq \delta)$$

For sufficiently large $n$. $\square$

In a sequence of statistical models $\cP_n = \{P_{n,\theta}: \theta \in \Theta\}$ with $X_n \sim P_{n,\theta}$, we say $\hat{\theta}_n$ is consistent for $g(\theta)$ if $\hat{\theta}_n \xrightarrow{p} g(\theta)$, meaning:

$$\mathbb{P}_\theta(|\hat{\theta}_n - g(\theta)| > \epsilon) \to 0$$

Usually, we omit the index $n$; sequence is implicit.

### Law of Large Numbers (LLN)

Let $\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$

If $\mathbb{E}|X_i| < \infty$, $\mathbb{E}X_i = \mu$, then $\bar{X}_n \xrightarrow{p} \mu$

### Central Limit Theorem (CLT)

If $\text{Var}(X_i) = \sigma^2 < \infty$, then $\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} N(0, \sigma^2)$

There are stronger versions of both the LLN and CLT, but this will generally be enough for us.

## Continuous Mapping Theorem

Theorem (Continuous Mapping): Let $g$ be continuous, $X_n, X$ r.v.'s.

1. If $X_n \xrightarrow{d} X$, then $g(X_n) \xrightarrow{d} g(X)$
2. If $X_n \xrightarrow{p} c$, then $g(X_n) \xrightarrow{p} g(c)$

Proof:
$f$ bounded continuous $\implies f \circ g$ bounded continuous
If $X_n \xrightarrow{d} X$, then $\mathbb{E}[f(g(X_n))] \to \mathbb{E}[f(g(X))]$
$X_n \xrightarrow{p} c$ special case with $X \equiv c$

## Slutsky's Theorem

Theorem (Slutsky): Assume $X_n \xrightarrow{d} X$, $Y_n \xrightarrow{p} c$. Then:

1. $X_n + Y_n \xrightarrow{d} X + c$
2. $X_n Y_n \xrightarrow{d} cX$
3. $X_n / Y_n \xrightarrow{d} X/c$ if $c \neq 0$

Proof: Show $(X_n, Y_n) \xrightarrow{d} (X, c)$, apply continuous mapping.

Wouldn't normally be true that $X_n \xrightarrow{d} X$, $Y_n \xrightarrow{d} Y$ implies $X_n + Y_n \xrightarrow{d} X + Y$ without specifying joint dist.

## Delta Method

Theorem (Delta Method):
If $\sqrt{n}(X_n - \mu) \xrightarrow{d} N(0, \sigma^2)$
$f(x)$ differentiable at $x = \mu$

Then $\sqrt{n}(f(X_n) - f(\mu)) \xrightarrow{d} N(0, [f'(\mu)]^2 \sigma^2)$

Instant: $X \sim N(\mu, \sigma^2/n) \implies f(X) \sim N(f(\mu), [f'(\mu)]^2 \sigma^2/n + o(1/n))$

Proof:
$f(X_n) = f(\mu) + f'(\mu)(X_n - \mu) + o(X_n - \mu)$
$\sqrt{n}(f(X_n) - f(\mu)) = f'(\mu)\sqrt{n}(X_n - \mu) + \sqrt{n}o(X_n - \mu)$
$N(0, \sigma^2) + 0 \xrightarrow{d} N(0, [f'(\mu)]^2 \sigma^2)$

Multivariate: $\sqrt{n}(X_n - \mu) \xrightarrow{d} N(0, \Sigma)$, $f: \mathbb{R}^d \to \mathbb{R}^k$
Derivative $Df(\mu)$ exists at $\mu$

Then $\sqrt{n}(f(X_n) - f(\mu)) \xrightarrow{d} N(0, Df(\mu) \Sigma Df(\mu)^T)$

$N(f(\mu), Df(\mu) \Sigma Df(\mu)^T/n + o(1/n))$ if $k=1$

### Example: Delta Method Application

$X_1, \ldots, X_n \sim \text{Unif}[0, \theta]$ iid
$Y_1, \ldots, Y_m \sim \text{Unif}[0, \theta]$ iid
$X, Y$ independent

For large $n, m$, what is the distribution of $T = \frac{\bar{X}}{\bar{Y}}$?

1. $\sqrt{n}(\bar{X} - \frac{\theta}{2}) \xrightarrow{d} N(0, \frac{\theta^2}{12})$ as $n \to \infty$
2. $\sqrt{m}(\bar{Y} - \frac{\theta}{2}) \xrightarrow{d} N(0, \frac{\theta^2}{12})$ as $m \to \infty$

$T_n = \frac{\bar{X}}{\bar{Y}} = \frac{\theta/2}{\theta/2} = 1 + O_p(n^{-1/2} + m^{-1/2})$

Let $f(x,y) = x/y$

$f_x'(\frac{\theta}{2}, \frac{\theta}{2}) = \frac{1}{\theta/2} = \frac{2}{\theta}$
$f_y'(\frac{\theta}{2}, \frac{\theta}{2}) = -\frac{\theta/2}{(\theta/2)^2} = -\frac{2}{\theta}$

$f'(\frac{\theta}{2}, \frac{\theta}{2}) = (\frac{2}{\theta}, -\frac{2}{\theta})$

$\sqrt{n}(T_n - 1) \xrightarrow{d} N(0, \frac{4}{\theta^2} \cdot \frac{\theta^2}{12} \cdot \frac{1}{n} + \frac{4}{\theta^2} \cdot \frac{\theta^2}{12} \cdot \frac{n}{m})$

$= N(0, \frac{1}{3n} + \frac{1}{3m})$

More accurate:
$\sqrt{n}(T_n - 1) \xrightarrow{d} N(0, \frac{4}{3}(1 + \frac{n}{m}))$

### What if $\mu = 0$?

3. What if $\mu_1 = \mu_2 = 0$? Conclusion still holds:
   $T_n = \frac{1 + O_p(n^{-1/2})}{1 + O_p(m^{-1/2})} = 1 + O_p(n^{-1/2} + m^{-1/2})$

Note: $\frac{1}{1 + n^{-1/2}} \to 1$ (continuous mapping)
Not Slutsky

So $n(T_n - 1)^2 \xrightarrow{d} \chi^2_1$ (continuous mapping)
Why not delta method?

In general, can do higher-order Taylor expansions for delta method if derivatives $\neq 0$:

$f(X_n) = f(\mu) + f'(\mu)(X_n - \mu) + \frac{1}{2}f''(\mu)(X_n - \mu)^2 + O_p(n^{-3/2})$

If $f'(\mu) = 0$, use second-order term:
$n(f(X_n) - f(\mu)) \xrightarrow{d} \frac{1}{2}f''(\mu)\chi^2_1$