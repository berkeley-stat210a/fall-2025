---
title: "Maximum Likelihood Estimation: Theory and Asymptotics"
date: "2023-11-14"
format:
  html:
    toc: true
    number-sections: true
---

{{< include latex-macros.tex >}}

## Maximum Likelihood Estimation

For a generic dominated family $\cP = \{P_\theta: \theta \in \Theta\}$ with densities $f_\theta$, a simple estimator for $\theta$ is:

$$\hat{\theta}_{\text{MLE}}(X) = \arg\max_{\theta \in \Theta} p_\theta(X) = \arg\max_{\theta \in \Theta} \prod_{i=1}^n f_\theta(X_i) = \arg\max_{\theta \in \Theta} \ell_n(\theta; X)$$

Remarks:
1. $\arg\max$ may not exist, be unique, or be computable
2. Doesn't depend on parameterization or base measure; MLE for $g(\theta)$ is $g(\hat{\theta}_{\text{MLE}})$

### Example: Exponential Family

$$\ell(\eta; X) = \eta^T T(X) - A(\eta) + \log h(X)$$

$T(\bar{X}) = \mathbb{E}_\eta[T(X)]$ if such $\eta$ exists

Because $\ell''(\eta; X) = -\text{Var}_\eta[T(X)]$ is negative definite unless $\eta \to T(\eta)$ constant, in which case param redundant

At most 1 solution exists

Let $m(X) = \mathbb{E}_\eta[T(X)] = \nabla A(\eta)$

### Example: Normal Distribution

$X_i \sim \text{iid } N(\theta, \sigma^2)$, $h(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-x^2/(2\sigma^2)}$, $\eta \in \mathbb{R}$

$T(X) = \frac{X}{\sigma^2}$, $A(\eta) = \frac{\eta^2}{2\sigma^2}$

Assume $\eta = \theta/\sigma^2$, $\sigma^2$ known

$m(\eta) = \eta\sigma^2 = \theta$, $m^{-1}(\theta) = \theta/\sigma^2$

Consistency: $\bar{X} \xrightarrow{p} \theta$ (LLN)

Cts mapping: $\hat{\eta} = m^{-1}(\bar{X}) \xrightarrow{p} \theta/\sigma^2$

Since $\sqrt{n}(\bar{X} - \theta) \xrightarrow{d} N(0, \text{Var}_\eta[T(X)])$:

$N(0, \sigma^2)$

Recall: $J(\eta) = \text{Var}_\eta[T(X)]$

Delta method: $\sqrt{n}(\hat{\eta} - \eta) \xrightarrow{d} N(0, [m^{-1'}(\theta)]^2 \sigma^2)$

$N(0, 1/\sigma^2)$

Recall: $J(\eta) = \text{Var}_\eta[T(X)] = \sigma^2$

$N(0, J^{-1})$

Asymptotically unbiased Gaussian, achieves CRLB

### Example: Poisson Distribution

$X_i \sim \text{iid Poisson}(\theta)$, $\eta = \log \theta$

$T(X) = X$, $\mathbb{E}[X] = \theta$, $N(0, \theta)$

$\hat{\eta}_n = \log \bar{X}$, $\sqrt{n}(\log \bar{X} - \log \theta) \xrightarrow{d} N(0, \theta^{-1})$ (Delta method)

$N(0, \theta^{-1})$

But for finite $n$, $\mathbb{P}(\bar{X} = 0) = \mathbb{P}(X_1 = 0)^n = e^{-n\theta} > 0$

MLE can have embarrassing finite sample performance despite being asymptotically optimal

### Proof: Convergence in Distribution with Probability Approaching 1

If $\mathbb{P}(B_n) \to 1$, $X_n \xrightarrow{d} X$, $Z_n$ arbitrary, then $X_n 1_{B_n} + Z_n 1_{B_n^c} \xrightarrow{d} X$

Proof: $\mathbb{P}(\|Z_n 1_{B_n^c}\| > \epsilon) \leq \mathbb{P}(B_n^c) \to 0$, so $Z_n 1_{B_n^c} \xrightarrow{p} 0$
Also, $1_{B_n} \xrightarrow{p} 1$, apply Slutsky

Any zany behavior has no effect on convergence in distribution

## Asymptotic Efficiency

In the exponential family case, generalizes to a much broader class of models

Setting: $X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} p_\theta(x)$, $\theta \in \mathbb{R}^d$

$p_\theta$ smooth in $\theta$ (e.g., 2 cts integrable derives, can be relaxed)

Let $\ell_i(\theta; X) = \log p_\theta(X_i)$, $\ell_n(\theta; X) = \sum_{i=1}^n \ell_i(\theta; X)$

$S_n(\theta) = \nabla_\theta \ell_n(\theta; X)$, $J_n(\theta) = \text{Var}_\theta[\nabla_\theta \ell_n(\theta; X)] = nJ_1(\theta)$

We say an estimator is asymptotically efficient if $\sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow{d} N(0, J_1^{-1}(\theta))$

Delta method for differentiable estimand $g(\theta)$:

$\sqrt{n}(g(\hat{\theta}_n) - g(\theta)) \xrightarrow{d} N(0, \nabla g(\theta)^T J_1^{-1}(\theta) \nabla g(\theta))$

Also achieves CRLB if $\hat{\theta}_n$ does, $g$ diff

## Asymptotic Distribution of MLE

Under mild conditions, $\hat{\theta}_n$ is asymptotically Gaussian efficient

We will be interested in $\ell_n(\theta; X)$ as a function of $\theta$
Notate true value as $\theta_0$: $X \sim P_{\theta_0}$

Derivatives of $\ell_n$ at $\theta_0$: $S_0$, $S_1$

$S_n(\theta_0; X) = \sum_{i=1}^n \nabla \ell_i(\theta_0; X_i) \sim N(0, J_n(\theta_0))$

$\mathbb{E}[S_n(\theta; X)] = n\mathbb{E}[\nabla \ell_i(\theta_0; X_i)] = 0$

$\mathbb{E}[-\nabla^2 \ell_n(\theta; X)] = \mathbb{E}[\sum_{i=1}^n -\nabla^2 \ell_i(\theta_0; X_i)] = J_n(\theta_0)$

### Informal Proof

Taylor expansion between $\theta_0$, $\hat{\theta}_n$:

$S_n(\hat{\theta}_n; X) = S_n(\theta_0; X) + S_n'(\theta_0; X)(\hat{\theta}_n - \theta_0)$

$\sqrt{n}(\hat{\theta}_n - \theta_0) = J_n^{-1}(\theta_0) S_n(\theta_0; X)/\sqrt{n}$

$\xrightarrow{d} N(0, J_1^{-1}(\theta_0))$

More rigorous proof later, but note we need consistency of $\hat{\theta}_n$ first to even justify Taylor expansion

### Quadratic Approximation

Quadratic approximation near $\theta_0$:

$\ell_n(\theta) \approx \ell_n(\theta_0) + (\theta - \theta_0)^T S_n(\theta_0) - \frac{1}{2}(\theta - \theta_0)^T J_n(\theta_0)(\theta - \theta_0)$

$N(J_n^{-1}(\theta_0)S_n(\theta_0), J_n^{-1}(\theta_0))$

Gaussian linear term + Deterministic curvature

$\ell_n(\theta) - \ell_n(\theta_0) \approx -\frac{n}{2}(\theta - \hat{\theta}_n)^T J_1(\theta_0)(\theta - \hat{\theta}_n) + \text{const}$

## Consistency of MLE

$X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} f_{\theta_0}$, $\theta_n \in \arg\max_{\theta \in \Theta} \ell_n(\theta; X)$

Will be ok if $\theta_n$ comes close to maximizing $\ell_n$

Question: When does $\ell_n \to \ell$?

Assume model identifiable: $P_\theta = P_{\theta_0}$ for $\theta \neq \theta_0$

Recall KL Divergence:

$D(g\|f) = \mathbb{E}_g[\log \frac{g(X)}{f(X)}] = \int g(x) \log \frac{g(x)}{f(x)} dx$ (note switch)

$\log \frac{g}{f} \geq 1 - \frac{f}{g}$ (strict ineq unless const, i.e., unless $f=g$)

Let $W_i(\theta) = \ell_i(\theta; X_i) - \ell_i(\theta_0; X_i)$, $W(\theta) = \mathbb{E}_{\theta_0}[W_i(\theta)]$

Note: $\theta_0 = \arg\max_{\theta \in \Theta} W(\theta)$

$W(\theta_0) = 0$

$W(\theta) = -\mathbb{E}_{\theta_0}[\log \frac{f_{\theta_0}(X)}{f_\theta(X)}] = -D(f_{\theta_0}\|f_\theta) \leq 0$

$= 0$ iff $\theta = \theta_0$

But not enough:
1. MLE $\hat{\theta}_n$ depends on entire function $\ell_n$
2. Need uniform convergence in $\theta$

### Definition: Compact Convergence

For compact $K$, let $C(K) = \{f: K \to \mathbb{R} \text{ cts}\}$

For $f \in C(K)$, let $\|f\| = \sup_{x \in K} |f(x)|$

$f_n \to f$ in this norm if $\|f_n - f\| \to 0$

### Theorem: LLN for Random Functions

Assume $K$ compact, $W_i, W_2, \ldots \in C(K)$ iid
$\mathbb{E}[\|W_i\|] < \infty$, $\mathbb{E}[W_i(\theta)] = W(\theta)$

Then $\bar{W}_n \in C(K)$
and $\mathbb{P}(\|\bar{W}_n - W\| > \epsilon) \to 0$

i.e., $\ell_n \to \ell$ in $\|\cdot\|_\infty$ norm

### Theorem (Keener 9.4)

Let $G_n, G$ random functions in $K$ compact
1. $G_n \to g$ in $\|\cdot\|_\infty$, some fixed $g \in C(K)$. Then:
   a. If $t^* \in K$ fixed, then $G_n(t^*) \xrightarrow{p} g(t^*)$
   b. If $g$ maximized at unique value $t^*$
      and $G_n(t_n) = \max_t G_n(t)$, then $t_n \xrightarrow{p} t^*$
2. If $K \subset \mathbb{R}$, $g'(t) = 0$ has unique sol. $t^*$
   and $t_n$ solve $G_n'(t) = 0$, then $t_n \xrightarrow{p} t^*$

(Sketch of proof in purple)
If $G_n'(t_n) = 0$, get $G_n'(t^*) = g'(t^*) + (g'(t_n) - g'(t^*)) + (G_n'(t^*) - g'(t^*))$
$\to 0 + 0 + 0$
by assumptions + by cts mapping

Fix $\epsilon > 0$, let $B_\epsilon = \{t: \|t - t^*\| < \epsilon\}$
Let $K_\epsilon = K \setminus B_\epsilon(t^*)$, $K \setminus B_\epsilon$ compact
$\delta = g(t^*) - \max_{t \in K_\epsilon} g(t) > 0$

If $t_n \notin K_\epsilon$, then $G_n(t_n) > G_n(t^*) - \frac{\delta}{2} > g(t^*) - \delta = \max_{t \in K_\epsilon} g(t)$

$\mathbb{P}(t_n \notin K_\epsilon) \leq \mathbb{P}(\|G_n - g\|_\infty > \frac{\delta}{2}) \to 0$

Analogous to $\bar{X}_n \xrightarrow{p} \mu$

### Theorem: Consistency of MLE for Compact $\Theta$

$X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} f_{\theta_0}$, $\cP$ has densities $p_\theta$, $\theta \in \Theta$

Assume:
1. $p_\theta$ cts in $\theta$
2. $\Theta$ compact
3. $\mathbb{E}_{\theta_0}[\sup_\theta |f_\theta(X)|/f_{\theta_0}(X)] < \infty$
4. $\mathbb{E}_{\theta_0}[\sup_\theta |W_i(\theta)|] < \infty$
5. Model identifiable

Then $\hat{\theta}_n \xrightarrow{p} \theta_0$ if $\hat